{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99f834bd-bcc5-4193-af16-aa68bfbd1caf",
   "metadata": {},
   "source": [
    "# 0. PIP Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3499f90a-2d91-4396-878c-17d8a2919628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pygame\n",
    "#!pip install numpy\n",
    "#!pip install gymnasium\n",
    "#!pip install pytorch (CHOOSE YOUR BEST OPTION ON WEB [CUDA, PLATFORM, ETC..]\n",
    "#!pip install stable-baselines3[extra]\n",
    "#!pip install optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d5c60d-7a6f-425c-a7f0-28e26dac2c89",
   "metadata": {},
   "source": [
    "Last tested was running on Machine with this version state:\n",
    "\n",
    "- PyGame: 2.5.2\n",
    "- OS: Windows-10\n",
    "- Python: 3.10.7\n",
    "- multi_sb3 [Stable-Baselines3: 2.1.0]\n",
    "- PyTorch: 2.1.0+cu121\n",
    "- GPU Enabled: True\n",
    "- Numpy: 1.25.0\n",
    "- Cloudpickle: 3.0.0\n",
    "- Gymnasium: 0.29.1\n",
    "- OpenAI Gym: 0.26.2\n",
    "- Optuna: 3.4.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a655f032-41e9-4c90-b524-70a1a32a5edd",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59574478-19a7-4e2a-9f00-df459dbc51dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multigamepy\n",
    "import paratroopergame\n",
    "import math\n",
    "# Import environment base class for a wrapper \n",
    "from gymnasium import Env \n",
    "\n",
    "# Import the space shapes for the environment\n",
    "from gymnasium.spaces import Discrete, Box, MultiBinary\n",
    "# Import numpy to calculate frame delta \n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import MultiSB3, PPO, DQN\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "import os\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, VecTransposeImage\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3aaf37-fa04-42e6-ba5a-d8299f494177",
   "metadata": {},
   "source": [
    "# 2. Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e76f3ec-e9ea-4bd0-87cb-010f1746e26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './logs/'\n",
    "OPT_DIR = './models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3754748-6fd7-4ff0-8330-58b62cbb4d87",
   "metadata": {},
   "source": [
    "# 3. MultiGame Manager (for VecEnv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a0da5e-1d15-4606-9f67-d514bdaff670",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTIGAME = multigamepy.MultiGameManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ac283c-b644-49d3-8aeb-f26a81bc508e",
   "metadata": {},
   "source": [
    "# 4. (Optional) Normal Gaming (Play yourself, not necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e870903-8c97-402f-b0ce-7555efd0bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Env wrapper not necessary here.  SHOOT WITH SPACEBAR!\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "game_instance = paratroopergame.GameSystem('Paratrooper game', paratroopergame.GAME_MODE_NORMAL, 'human')\n",
    "MULTIGAME.regGame(game_instance)\n",
    "\n",
    "game_instance.reset()\n",
    "\n",
    "count = 0\n",
    "\n",
    "while(game_instance.isRunning()):\n",
    "    retVal = game_instance.step()\n",
    "    game_instance.render()\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    if(count % 60 == 0):\n",
    "        pass\n",
    "        #plt.figure()\n",
    "        #plt.imshow(retVal[0], cmap='gray', vmin=0, vmax=255)\n",
    "        #plt.show()\n",
    "    \n",
    "game_instance.close()\n",
    "MULTIGAME.unregGame(game_instance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4988aad0-987b-4b0a-88b8-23493055ca60",
   "metadata": {},
   "source": [
    "# 4. Game Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f940cf-bdc7-4eb4-b67d-b126609b5d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paratrooper(Env): \n",
    "    def __init__(self, render_mode = 'human'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Startup and instance of the game \n",
    "        self.game = paratroopergame.GameSystem('Paratrooper game', paratroopergame.GAME_MODE_EXT_ACTION, render_mode)\n",
    "\n",
    "        # Specify action space and observation space\n",
    "        self.render_mode = render_mode\n",
    "        self.observation_space = Box(low=0, high=255, shape=(45, 80, 1), dtype=np.uint8)\n",
    "        self.action_space = MultiBinary(3)\n",
    "    \n",
    "    def reset(self, seed = 0):        \n",
    "        # Return the first frame \n",
    "        obs = [self.game.reset(seed)]\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        info['none'] = 0\n",
    "\n",
    "        self.LastElapsedTime = 0\n",
    "        self.LastDestroyedParatroopers = 0\n",
    "        self.LastMissedBullets = 0\n",
    "        self.LastEscapedParatroopers = 0\n",
    "        \n",
    "        return obs, info\n",
    "    \n",
    "    def step(self, action): \n",
    "        # Take a step \n",
    "        obs, done, trimmed, info = self.game.step(action)\n",
    "\n",
    "        # There is only one observation element\n",
    "        obs = [obs]\n",
    "\n",
    "        reward = [0,0]\n",
    "\n",
    "        reward[0] = info['ElapsedTime'] - self.LastElapsedTime\n",
    "        reward[0] += (info['DestroyedParatroopers'] - self.LastDestroyedParatroopers) * 10\n",
    "\n",
    "        if(not info['LowestParatrooperExists']):\n",
    "            targetVector = [0,-1]\n",
    "        else:\n",
    "            _lp = info['LowestParatrooperPosition']\n",
    "            _cp = info['CannonPosition']\n",
    "            _ccs = info['CannonAngleCosSin']\n",
    "            targetVector = [_lp[0] - _cp[0], _lp[1] - _cp[0]]\n",
    "\n",
    "            #If paratrooper is lower than cannon, its impossible to reach\n",
    "            if(targetVector[1] > 0):\n",
    "                targetVector[1] = 0\n",
    "\n",
    "        _dotproduct = _ccs[0] * targetVector[0] + _ccs[1] * targetVector[1]\n",
    "            \n",
    "        reward[1] = _dotproduct\n",
    "\n",
    "\n",
    "        self.LastElapsedTime = info['ElapsedTime']\n",
    "        self.LastDestroyedParatroopers = info['DestroyedParatroopers']\n",
    "        self.LastMissedBullets = info['MissedBullets']\n",
    "        self.LastEscapedParatroopers = info['EscapedParatroopers']\n",
    "        \n",
    "        return obs, reward, done, trimmed, info\n",
    "    \n",
    "    def render(self, *args, **kwargs):\n",
    "        self.game.render()\n",
    "        \n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259e41b1-f954-4681-b647-3f1496accfe6",
   "metadata": {},
   "source": [
    "# 5. Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ee976d-bac0-4608-9b32-180aba67dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return test hyperparameters - define the object function\n",
    "def optimize_ppo(trial): \n",
    "    return {\n",
    "        'n_steps':trial.suggest_int('n_steps', 32, 8192),\n",
    "        'gamma':trial.suggest_float('gamma', 0.8, 0.9999, log=True),\n",
    "        'learning_rate':trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True),\n",
    "        'clip_range':trial.suggest_float('clip_range', 0.1, 0.4),\n",
    "        'gae_lambda':trial.suggest_float('gae_lambda', 0.8, 0.99)\n",
    "    }\n",
    "\n",
    "def optimize_dqn(trial): \n",
    "    return {\n",
    "        'buffer_size':trial.suggest_int('buffer_size',64,8192, log=True),\n",
    "        'learning_starts':trial.suggest_int('learning_starts', 64, 10000),\n",
    "        'learning_rate':trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True),\n",
    "        'tau':trial.suggest_float('tau',0.85,1.0, log=True),\n",
    "        'gamma':trial.suggest_float('gamma', 0.8, 0.9999, log=True),\n",
    "        'train_freq':trial.suggest_int('train_freq', 4,128, log=True),\n",
    "        'gradient_steps':trial.suggest_int('gradient_steps', 1,8, log=True),\n",
    "        'target_update_interval':trial.suggest_int('target_update_interval', 2000,15000, log=True),\n",
    "\n",
    "        'exploration_fraction':trial.suggest_float('exploration_fraction', 0.1, 0.2),\n",
    "        'exploration_final_eps':trial.suggest_float('exploration_final_eps', 0.1, 0.3),\n",
    "        'max_grad_norm':trial.suggest_float('max_grad_norm', 5, 15)\n",
    "    }\n",
    "\n",
    "SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89c0319-78d4-4bcb-bdc7-2644f8a2ca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a training loop and return mean reward \n",
    "def optimize_agent(trial):\n",
    "    try:\n",
    "        global env\n",
    "        model_params = optimize_dqn(trial) \n",
    "\n",
    "        # Create algo \n",
    "        model = DQN('MlpPolicy', env, tensorboard_log=LOG_DIR, verbose=0, **model_params)\n",
    "        model.learn(total_timesteps=50000)\n",
    "        #model.learn(total_timesteps=100000)\n",
    "\n",
    "        # Evaluate model \n",
    "        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "        env.reset()\n",
    "\n",
    "        SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(trial.number))\n",
    "        model.save(SAVE_PATH)\n",
    "\n",
    "        return mean_reward\n",
    "\n",
    "    except Exception as e:\n",
    "        return -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b86ad6-9a26-4b42-99a5-27369d65c702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment \n",
    "env = ParatrooperGym('ai')\n",
    "env = Monitor(env, LOG_DIR)\n",
    "# Creating the experiment \n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(optimize_agent, n_trials=40, n_jobs=1)\n",
    "env.close()\n",
    "#study.optimize(optimize_agent, n_trials=100, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c333d3-620f-4381-a4e3-5c715cc0b9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890d93b3-44d9-43c3-8747-2379920ad7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea69fc5-f26f-4db0-a6e2-1b0a727daa5e",
   "metadata": {},
   "source": [
    "# 5. Callback Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf0224d-ba95-4128-8b5e-ee3750f2a9d6",
   "metadata": {},
   "source": [
    "Don't worry if game window seems frozen, training is going on and can be checked on log folder during whole proccess. Don't close game window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1477a8fa-de3c-4c30-84f6-ce20ccf66ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class for periodic stoarge of trained models\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, save_path, algo_name, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.algo_name = algo_name\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}_{}'.format(self.algo_name,self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        # It is necessary to return True. Otherwise learn process would end\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f69a00-6e4a-4ae6-af83-da1d9bd2287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment. AI tells game not to render or process window events, which makes FPS higher\n",
    "real_env = Paratrooper('ai')\n",
    "real_env = Monitor(real_env, LOG_DIR)\n",
    "\n",
    "# Algorithm list with deferred actions would be next:\n",
    "# 0: DQN (shot)\n",
    "# 1: PPO (aim)\n",
    "observation_list = []\n",
    "action_space_list = []\n",
    "\n",
    "action_space_list.append(Discrete(2)) # DQN (DON'T SHOT, Action SHOT), DQN can only cope with Discrete Spaces\n",
    "action_space_list.append(MultiBinary(2)) # PPO (Action Right, Left)\n",
    "observation_list.append(real_env.observation_space) # DQN will observe element 0 of obs return\n",
    "observation_list.append(real_env.observation_space) # PPO will observe element 0 of obs return (Same as DQN)\n",
    "\n",
    "# Every algorithm can take its own periodicity to save model\n",
    "callback_0 = TrainAndLoggingCallback(check_freq=20000, save_path=OPT_DIR, algo_name='DQN')\n",
    "callback_1 = TrainAndLoggingCallback(check_freq=20000, save_path=OPT_DIR, algo_name='PPO')\n",
    "\n",
    "# Create virtual environments\n",
    "virtual_env_list = MultiSB3.createVirtualEnvironments(numAlgorithms=2, observationSpaceList=observation_list, actionSpaceList=action_space_list)\n",
    "\n",
    "# Create algorithms, by association of its indexed virtual environment with them\n",
    "alg_0 = DQN('CnnPolicy', virtual_env_list[0], tensorboard_log=LOG_DIR)\n",
    "alg_1 = PPO('CnnPolicy', virtual_env_list[1], tensorboard_log=LOG_DIR)\n",
    "#alg_0 = DQN.load(os.path.join(OPT_DIR, 'best_model_DQN_x.zip'), env=env)\n",
    "#alg_1 = PPO.load(os.path.join(OPT_DIR, 'best_model_PPO_x.zip'), env=env)\n",
    "\n",
    "# Dictionary for DQN will specify DQN model itself, and obs_index 0 to pick first element from real environment return\n",
    "alg_collection_0 = {}\n",
    "alg_collection_0['alg'] = alg_0\n",
    "alg_collection_0['obs_index'] = 0\n",
    "alg_collection_0['reward_index'] = 0\n",
    "alg_collection_0['action_indexes'] = [0]\n",
    "alg_collection_0['action_space'] = action_space_list[0]\n",
    "\n",
    "# Dictionary for PPO will specify PPO model itself, and obs_index 0 to pick first element from real environment return\n",
    "alg_collection_1 = {}\n",
    "alg_collection_1['alg'] = alg_1\n",
    "alg_collection_1['obs_index'] = 0\n",
    "alg_collection_1['reward_index'] = 1\n",
    "alg_collection_1['action_indexes'] = [1,2]\n",
    "alg_collection_1['action_space'] = action_space_list[1]\n",
    "\n",
    "alg_collection = [alg_collection_0, alg_collection_1]\n",
    "\n",
    "# Create MultiAlgorithm, this one is in contact with real environment and deffers actions and observation to virtual environments\n",
    "model = MultiSB3(real_env, alg_collection, virtual_env_list)\n",
    "\n",
    "#model.learn(total_timesteps=1000000, callback=callback)\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769373d3-c0f0-4f66-8357-11e133a3b204",
   "metadata": {},
   "source": [
    "# 6. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e78b78f-a8f7-41fa-8a1b-3edece8e2ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human render tells game to render and process window event. Also render variable in evaluate_policy tells to call Env render() function periodically\n",
    "env = ParatrooperGym('human')\n",
    "env = Monitor(env, LOG_DIR)\n",
    "model = DQN.load(os.path.join(OPT_DIR, 'best_model_200000.zip'))\n",
    "mean_reward, _ = evaluate_policy(model, env, render=True, n_eval_episodes=15)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
